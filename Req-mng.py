import os
from dotenv import load_dotenv
import streamlit as st
import pandas as pd
from openai import AzureOpenAI  

#from langchain_community.chat_models import ChatOpenAI
#from langchain.schema import HumanMessage

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
load_dotenv()
# í™˜ê²½ ë³€ìˆ˜ì—ì„œ OpenAI ë° Azure ì„¤ì • ë¶ˆëŸ¬ì˜¤ê¸°
openai_endpoint = os.getenv("OPENAI_ENDPOINT")
openai_api_key = os.getenv("OPENAI_API_KEY")
chat_model = os.getenv("CHAT_MODEL")
embedding_model = os.getenv("EMBEDDING_MODEL")
search_endpoint=os.getenv("SEARCH_ENDPOINT")
search_api_key=os.getenv("SEARCH_API_KEY")
index_name = os.getenv("INDEX_NAME")     


#print("openai_end:", openai_endpoint)
#print("search_endpoint:", search_endpoint)
#print("chat_model:", chat_model)
print("embedding_model:", embedding_model)


openai_endpoint = "https://hsw-openai1.openai.azure.com/"
openai_api_key = "DQ6cfI9qSnjkiozlfWu55qiffjZeAmzo6Jy6UM9T7BgGSVvUFJY5JQQJ99BFACMsfrFXJ3w3AAABACOG6PI1"
chat_model = "hsw-4o-mini"
embedding_model = "hsw-embedding-3-small"
search_endpoint="https://hsw-aisearch.search.windows.net"
search_api_key="YcMqjxzjE58KpysnSvN3Wz2yX9nWdkY32p5Ww2L7CQAzSeBOgw97"
index_name = "hsw-rag-1"
#index_name = "hswazureblob-index"

print("openai_end:", openai_endpoint)
print("search_endpoint:", search_endpoint)
print("chat_model:", chat_model)
print("index_name", index_name)
print("embedding_model:", embedding_model)


# ğŸ“Œ í˜ì´ì§€ ì„¤ì • (ê°€ì¥ ì²« ì¤„ì—ì„œ í˜¸ì¶œ)
st.set_page_config(page_title="ìš”êµ¬ì‚¬í•­ & ê°œë°œì í†µê³„",layout="wide")

# ì—‘ì…€ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°
@st.cache_data
def load_data():
    df = pd.read_excel("ìš”êµ¬ì‚¬í•­.xlsx")
    return df

df = load_data()

st.header("ğŸ“Š ìš”êµ¬ì‚¬í•­ í†µê³„ & ê°œë°œìë³„ ì§„í–‰ë¥ ")

# ìƒë‹¨: ìš”êµ¬ì‚¬í•­ í†µê³„ ë° ê°œë°œìë³„ í†µê³„ (ê°€ë¡œ ë°°ì¹˜)
col1, col2, col3, col4 = st.columns(4, gap="small")
#col1, col2 = st.columns(2, gap="small")
#col3, col4 = st.columns(2, gap="small")
# ê·¸ë˜í”„ í¬ê¸° ì„¤ì •
graph_width = 300
graph_height = 400

# ìš”êµ¬ì‚¬í•­ í†µê³„ ë° ê°œë°œìë³„ í†µê³„
with col1:
    st.subheader("ğŸ“Œ ìš”êµ¬ì‚¬í•­ ì²˜ë¦¬ ìƒíƒœë³„ ê±´ìˆ˜")
    status_counts = df["ì²˜ë¦¬ìƒíƒœ"].value_counts()
    st.bar_chart(status_counts,width=graph_width, height=graph_height,use_container_width=False)

with col2:
    st.subheader("ğŸ‘¨â€ğŸ’» ì„œë¹„ìŠ¤ë³„ ë¯¸ì ‘ìˆ˜ ë‚´ì—­")
    requst_status = df[df["ì²˜ë¦¬ìƒíƒœ"] == "1.ìš”ì²­"]
    requst_counts = requst_status["ê³ ê°ì„œë¹„ìŠ¤"].value_counts()
    st.bar_chart(requst_counts,width=graph_width, height=graph_height,use_container_width=False)

with col3:
    st.subheader("ğŸ‘¨â€ğŸ’» ë¶„ì„ê°€ë³„ ë¶„ì„ ê±´ìˆ˜")
    received = df[df["ì²˜ë¦¬ìƒíƒœ"] == "2.ì ‘ìˆ˜"]
    analyst_counts = received["ë¶„ì„ê°€"].value_counts()
    st.bar_chart(analyst_counts,width=graph_width, height=graph_height,use_container_width=False)

with col4:
    st.subheader("ğŸ‘¨â€ğŸ’» ê°œë°œìë³„ ê°œë°œì¤‘ ê±´ìˆ˜")
    dev_in_progress = df[df["ì²˜ë¦¬ìƒíƒœ"] == "3.ê°œë°œì¤‘"]
    developer_counts = dev_in_progress["ê°œë°œì"].value_counts()
    st.bar_chart(developer_counts,width=graph_width, height=graph_height,use_container_width=False)


# ì±—ë´‡ ì„¤ì •
chat_client = AzureOpenAI(
    api_version="2024-12-01-preview",
    azure_endpoint=openai_endpoint,
    api_key=openai_api_key
)

#chat í™”ë©´

if "messages" not in st.session_state:
    st.session_state.messages = [
        {
            "role": "system",
            "content": "ìš”êµ¬ì‚¬í•­ì€ ìš”ì²­ì´ ì˜¤ë©´ ìš”ì²­ë‹¨ê³„ì—ì„œ ë¶„ì„ê°€ê°€ ì ‘ìˆ˜í•˜ë©´ ì ‘ìˆ˜ë‹¨ê³„ì—ì„œ ê°œë°œìê°€ ê°œë°œì¤‘ì´ë©´ ê°œë°œë‹¨ê³„ì—ì„œ ìš”êµ¬ì‚¬í•­ì´ ì™„ë£Œë˜ë©´ ì™„ë£Œ ë‹¨ê³„ì´ë©°, ì‚¬ìš©ìì˜ ë¬¼ìŒì— ìš”êµ¬ì‚¬í•­ ë°ì´í„°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë„ì›€ì„ ì£¼ëŠ” ì±—ë´‡ì´ê³  ëª¨ë¥´ë©´ ë³´ë¥¸ë‹¤ê³  ë‹µ ë“œë¦¬ê² ìŠµë‹ˆë‹¤."
        },
    ]

# Display chat history
for message in st.session_state.messages:
    st.chat_message(message["role"]).write(message["content"],)

st.subheader("ì•ˆë…•í•˜ì„¸ìš”! ìš”êµ¬ì‚¬í•­ ê´€ë¦¬ ì±—ë´‡ì…ë‹ˆë‹¤. ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?")

def get_openai_response(messages):
    # Additional parameters to apply RAG pattern using the AI Search index
    rag_params = {
        "data_sources": [
            {
                # he following params are used to search the index
                "type": "azure_search",
                "parameters": {
                    "endpoint": search_endpoint,
                    "index_name": index_name,
                    "authentication": {
                        "type": "api_key",
                        "key": search_api_key,                
                    },
                    # The following params are used to vectorize the query
                    "query_type": "vector",
                    #"vectorize_query": True,                                        
                    "embedding_dependency": {
                        "type": "deployment_name",
                        "deployment_name": embedding_model,
                    },
                }
            }
        ],
    }
    
    response = chat_client.chat.completions.create(
        model=chat_model,
        messages=messages,
        extra_body=rag_params
    )

    completion = response.choices[0].message.content
    return completion


if user_input := st.chat_input("ë¬¸ì˜ ì‚¬í•­ì„ ì…ë ¥í•´ì£¼ì„¸ìš”."):
    st.session_state.messages.append({"role": "user", "content": user_input})
    st.chat_message("user").write(user_input)

    with st.spinner("ì‘ë‹µì„ ê¸°ë‹¤ë¦¬ëŠ” ì¤‘..."):
        response = get_openai_response(st.session_state.messages)

    st.session_state.messages.append({"role": "assistant", "content": response})
    st.chat_message("assistant").write(response)
